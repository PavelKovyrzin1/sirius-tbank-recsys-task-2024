{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc44165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import ast\n",
    "\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5ec5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./dataset_additional/\"\n",
    "\n",
    "users_df = pd.read_csv(data_folder + \"users_df.csv\")\n",
    "items_df = pd.read_csv(data_folder + \"items_df.csv\")\n",
    "\n",
    "countries = pd.read_csv(data_folder + \"countries.csv\")\n",
    "genres = pd.read_csv(data_folder + \"genres.csv\")\n",
    "staff = pd.read_csv(data_folder + \"staff.csv\")\n",
    "\n",
    "data_folder = \"./\"\n",
    "\n",
    "train_part = pd.read_csv(data_folder + \"train_data.csv\", parse_dates=[\"datetime\"])\n",
    "test_part = pd.read_csv(data_folder + \"test_data.csv\")\n",
    "test_part = test_part.groupby(\"user_id\").agg({\"movie_id\": list}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0d50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender(ABC):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        # реализация может быть любой, никаких ограничений\n",
    "\n",
    "        # не забудьте про\n",
    "        self.trained = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, df: pd.DataFrame, topn: int = 10) -> List[np.ndarray]:\n",
    "        # реализация может быть любой, НО\n",
    "        # должен возвращать список массивов из movie_id, которые есть в `item_df`, чтобы корректно работал подсчет метрик\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340aba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHTUNG! DO NOT TOUCH \n",
    "\n",
    "def ndcg_metric(gt_items: np.ndarray, predicted: np.ndarray) -> float:\n",
    "    at = len(predicted)\n",
    "    relevance = np.array([1 if x in predicted else 0 for x in gt_items])\n",
    "    # DCG uses the relevance of the recommended items\n",
    "    rank_dcg = dcg(relevance)\n",
    "    if rank_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    # IDCG has all relevances to 1 (or the values provided), up to the number of items in the test set that can fit in the list length\n",
    "    ideal_dcg = dcg(np.sort(relevance)[::-1][:at])\n",
    "\n",
    "    if ideal_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    ndcg_ = rank_dcg / ideal_dcg\n",
    "\n",
    "    return ndcg_\n",
    "\n",
    "\n",
    "def dcg(scores: np.ndarray) -> float:\n",
    "    return np.sum(\n",
    "        np.divide(np.power(2, scores) - 1, np.log2(np.arange(scores.shape[0], dtype=np.float64) + 2)), dtype=np.float64\n",
    "    )\n",
    "\n",
    "\n",
    "def recall_metric(gt_items: np.ndarray, predicted: np.ndarray) -> float:\n",
    "    n_gt = len(gt_items)\n",
    "    intersection = len(set(gt_items).intersection(set(predicted)))\n",
    "    return intersection / n_gt\n",
    "\n",
    "\n",
    "def evaluate_recommender(df: pd.DataFrame, model_preds_col: str, gt_col: str = \"movie_id\") -> Dict[str, float]:\n",
    "    metric_values = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        metric_values.append(\n",
    "            (ndcg_metric(row[gt_col], row[model_preds_col]), recall_metric(row[gt_col], row[model_preds_col]))\n",
    "        )\n",
    "\n",
    "    return {\"ndcg\": np.mean([x[0] for x in metric_values]), \"recall\": np.mean([x[1] for x in metric_values])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034522ad",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8acc0",
   "metadata": {},
   "source": [
    "Посмотрим, как себя покажем классический KNN. Поскольку пользователей в датасете достаточно много, построить KNN на основе данных о всех пользователях было бы очень времязатратно. Поэтому построим модель на айтемах, которых на порядок меньше. Первая идея такая: для каждого пользователя из тестовых данных найдём топ-10 фильмов, расстояние до \"усредненных\" просмотренных пользователем фильмов которых минимально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38f2100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>description</th>\n",
       "      <th>genres</th>\n",
       "      <th>countries</th>\n",
       "      <th>staff</th>\n",
       "      <th>title_orig</th>\n",
       "      <th>age_rating</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Мама, я дома</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-11-23T00:00:00</td>\n",
       "      <td>Где-то в глубинке вместе с дочерью и внуком жи...</td>\n",
       "      <td>[97]</td>\n",
       "      <td>[238]</td>\n",
       "      <td>[1883, 33655, 25890, 1001, 12051, 10110, 16895]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Три метра над уровнем неба</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>История любви парня и девушки, принадлежащих к...</td>\n",
       "      <td>[138, 97, 294]</td>\n",
       "      <td>[242]</td>\n",
       "      <td>[18168, 23444, 10850, 21847, 30555, 24469, 268...</td>\n",
       "      <td>Tres metros sobre el cielo</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Три, метра, над, уровнем, неба, 2010, Испания,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       title        year     date_publication  \\\n",
       "0   0                Мама, я дома  2022-01-01  2022-11-23T00:00:00   \n",
       "1   1  Три метра над уровнем неба  2010-01-01                  NaN   \n",
       "\n",
       "                                         description          genres  \\\n",
       "0  Где-то в глубинке вместе с дочерью и внуком жи...            [97]   \n",
       "1  История любви парня и девушки, принадлежащих к...  [138, 97, 294]   \n",
       "\n",
       "  countries                                              staff  \\\n",
       "0     [238]    [1883, 33655, 25890, 1001, 12051, 10110, 16895]   \n",
       "1     [242]  [18168, 23444, 10850, 21847, 30555, 24469, 268...   \n",
       "\n",
       "                   title_orig  age_rating  \\\n",
       "0                         NaN         NaN   \n",
       "1  Tres metros sobre el cielo        16.0   \n",
       "\n",
       "                                            keywords  \n",
       "0                                                NaN  \n",
       "1  Три, метра, над, уровнем, неба, 2010, Испания,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4462fc9",
   "metadata": {},
   "source": [
    "Определимся, с какими признаками будем работать. Во-первых, логично сравнивать фильмы по жанру, стране и актерам/режиссёрам, т.к. это наиболее весомые признаки. Остальные признаки использовать не будем, т.к. они либо  не имеют отношения к схожести фильмов, либо коррелируют с выбранными признаками (например, year и staff). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93ba52",
   "metadata": {},
   "source": [
    "Приведём столбцы, с которыми будем работать, к спискам целых чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f017b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df['genres'] = [ast.literal_eval(genres) for genres in items_df['genres']]\n",
    "items_df['countries'] = [ast.literal_eval(countries) for countries in items_df['countries']]\n",
    "items_df['staff'] = [ast.literal_eval(staff) for staff in items_df['staff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3011cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(BaseRecommender):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.binarized_df = pd.DataFrame()\n",
    "        \n",
    "    def _add_bin_column(self, column):\n",
    "        # применяем one-hot кодирование\n",
    "        binarized_column = pd.DataFrame(self.mlb.fit_transform(items_df[column]))\n",
    "        \n",
    "        # чтобы признаков в итоге получилось не слишком много, выберем наиболее попурярные\n",
    "        sums = binarized_column.sum()\n",
    "        top_100_columns = sums.nlargest(100).index\n",
    "        binarized_column = binarized_column[top_100_columns]\n",
    "        \n",
    "        # дадим признакам веса в зависимости от того, насколько они популярны\n",
    "        max_sum = binarized_column.sum().max()\n",
    "        binarized_column = binarized_column.apply(lambda col: col * (col.sum() / max_sum) if max_sum != 0 else 0)\n",
    "        \n",
    "        self.binarized_df = pd.concat([self.binarized_df, pd.DataFrame(binarized_column)], axis=1)\n",
    "        \n",
    "    def _get_best(self, inds, n_neighbors=10):\n",
    "        if len(inds) == 0:\n",
    "            return []\n",
    "        \n",
    "        sum_row = self.binarized_df.loc[inds].sum()\n",
    "\n",
    "        distances, indices = self.knn_model.kneighbors([sum_row], n_neighbors=n_neighbors+len(inds))\n",
    "\n",
    "        answer = [index for index in indices[0] if index not in inds]\n",
    "        return answer[:n_neighbors]\n",
    "        \n",
    "\n",
    "    def fit(self, df: pd.DataFrame, train_df: pd.DataFrame, item_id_cols: list[int]) -> None:\n",
    "        for column in item_id_cols:\n",
    "            self._add_bin_column(column)\n",
    "            \n",
    "        self.binarized_df.columns = self.binarized_df.columns.astype(str)\n",
    "            \n",
    "        self.train_df = train_df.groupby(\"user_id\").agg({\"movie_id\": list}).reset_index()\n",
    "        self.train_df['movie_id'] = self.train_df['movie_id'].apply(lambda x: np.array([int(i) for i in x]))\n",
    "        \n",
    "        self.knn_model = NearestNeighbors()\n",
    "        self.knn_model.fit(self.binarized_df.values)\n",
    "            \n",
    "        self.trained = True\n",
    "        \n",
    "\n",
    "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        assert self.trained\n",
    "\n",
    "        return [self._get_best(list(list(self.train_df[self.train_df['user_id'] == id]['movie_id'])[0])) \n",
    "                for id in df['user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c15212",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN()\n",
    "knn.fit(items_df, train_part, ['genres', 'countries', 'staff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb47805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 45s\n",
      "Wall time: 16min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_part['knn_recs'] = knn.predict(test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d3d784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.022481548834698276, 'recall': 0.010370621777172335}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(df=test_part, model_preds_col=\"knn_recs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41901cc7",
   "metadata": {},
   "source": [
    "Значения метрик практически нулевые. Вероятно, это связано с тем, что мы используем мало признаков, а также практически не учитываем популярность фильмов, хотя, как мы поняли, это очень важный признак. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7c79c",
   "metadata": {},
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4170f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting(BaseRecommender):\n",
    "    def __init__(self, iterations: int = 1, learning_rate: float = 0.25, depth: int = 6):\n",
    "        super().__init__()\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.model = CatBoostRegressor(iterations=iterations, learning_rate=learning_rate, depth=depth, verbose=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Бинарные признаки закодируем как 0 и 1, а остальные признаки являются порядковыми, \n",
    "        # поэтому можно их просто равномерно нормировать\n",
    "\n",
    "\n",
    "        self._age_encoder = {\n",
    "            '18-24': 0.0,\n",
    "            '25-34': 0.2,\n",
    "            '35-44': 0.45,\n",
    "            '45-54': 0.69,\n",
    "            '55-70': 1.0\n",
    "        }\n",
    "\n",
    "        self._income_encoder = {\n",
    "            'низкий': 0.0,\n",
    "            'средний': 0.33,\n",
    "            'высокий': 0.67,\n",
    "            'очень высокий': 1.0\n",
    "        }\n",
    "        \n",
    "        self._sex_encoder = {\n",
    "            'Женский': 0.0,\n",
    "            'Мужской': 1.0\n",
    "        }\n",
    "        \n",
    "        self._education_encoder = {\n",
    "            'Без образования': 0.0,\n",
    "            'Среднее': 0.33,\n",
    "            'Неполное высшее': 0.67,\n",
    "            'Высшее': 1.0\n",
    "        }\n",
    "        \n",
    "        \n",
    "    # Закодируем категориальные признаки в соответсвтии со словарями, None заменим средними значениями\n",
    "    def _prepare_categorical_features(self) -> None:\n",
    "        self.df['age_category'] = self.df['age_category'].replace(self._age_encoder)\n",
    "        self.df['age_category'].fillna(self.df['age_category'].mean(), inplace=True)\n",
    "\n",
    "        self.df['income'] = self.df['income'].replace(self._income_encoder)\n",
    "        self.df['income'].fillna(self.df['income'].mean(), inplace=True)\n",
    "        \n",
    "        self.df['sex'] = self.df['sex'].replace(self._sex_encoder)\n",
    "        self.df['sex'].fillna(self.df['sex'].mean(), inplace=True)\n",
    "        \n",
    "        self.df['education'] = self.df['education'].replace(self._education_encoder)\n",
    "        self.df['education'].fillna(self.df['education'].mean(), inplace=True)\n",
    "        \n",
    "        self.df['kids_flg'].fillna(self.df['kids_flg'].mean(), inplace=True)\n",
    "        \n",
    "    def _prepare_movies(self) -> None:\n",
    "        # Для каждого пользователя оставим только информацию о самых популярных фильмах\n",
    "        self.df['movie_id'] = [[movie for movie in movies if movie in self.recommendations] for movies in self.df['movie_id']]\n",
    "        \n",
    "        # Применим one-hot кодирование\n",
    "        binarized_column = self.mlb.fit_transform(self.df['movie_id'])\n",
    "        binarized_column_df = pd.DataFrame(binarized_column, columns=self.mlb.classes_)\n",
    "\n",
    "        self.df = pd.concat([self.df, pd.DataFrame(binarized_column_df)], axis=1)\n",
    "\n",
    "        self.df.drop(columns=['movie_id'], inplace=True)\n",
    "        \n",
    "        \n",
    "    def _prepare(self) -> None:\n",
    "        self._prepare_categorical_features()\n",
    "        self._prepare_movies()\n",
    "\n",
    "    def fit(self, train_df: pd.DataFrame, items_df: pd.DataFrame) -> None:\n",
    "        self.recommendations = set(train_df['movie_id'].value_counts().index.values[:100])\n",
    "        \n",
    "        self.df = train_df.groupby(\"user_id\").agg({\"movie_id\": list}).reset_index()\n",
    "        self.df = pd.merge(self.df, users_df, on='user_id', how='left')\n",
    "        \n",
    "        self.grouped_df = train_df.groupby(\"user_id\").agg({\"movie_id\": list}).reset_index()\n",
    "        \n",
    "        self.matrix = pd.DataFrame(self.mlb.fit_transform(self.df['movie_id']), columns=self.mlb.classes_)\n",
    "        \n",
    "        self._prepare()\n",
    "        self.trained = True\n",
    "        \n",
    "    def _predict(self, ind) -> None: # Функция предсказания для пользователей с номерами от ind до ind+test_rows\n",
    "        current_test_df = self.test_df.iloc[ind: min(ind + self.test_rows, len(self.test_df))]\n",
    "        \n",
    "        # Разделим данные на одучающую и тестовую части\n",
    "        mask = self.df['user_id'].isin(current_test_df['user_id'])\n",
    "        \n",
    "        train_indices = self.df.index[~mask]\n",
    "        test_indices = self.df.index[mask]\n",
    "\n",
    "        X_train = self.df.iloc[train_indices]\n",
    "        X_test = self.df.iloc[test_indices]\n",
    "        \n",
    "        for movie in self.matrix.columns: # Для каждого фильма обучим модель и сделаем предсказание\n",
    "            target_matrix = self.matrix[[movie]]\n",
    "            \n",
    "            y_train = target_matrix.iloc[train_indices]\n",
    "            \n",
    "            # Если ни один пользователь из обучающей части не посмотрел фильм, то обучать модель нет смысла\n",
    "            if len(y_train.iloc[:, 0].unique()) == 1: \n",
    "                self.ratings.loc[ind:min(ind + self.test_rows, len(self.test_df)), y_train.columns] = 0\n",
    "                continue\n",
    "                \n",
    "            # Если фильм популярный, то он присутствует как признак. \n",
    "            # В таком случае его нужно удалить, чтобы корректно обучить модель\n",
    "            if movie in self.recommendations:\n",
    "                train_pool = Pool(data=X_train.drop(columns=[movie]), label=y_train)\n",
    "                test_pool = Pool(data=X_test.drop(columns=[movie]))\n",
    "            else:\n",
    "                train_pool = Pool(data=X_train, label=y_train)\n",
    "                test_pool = Pool(data=X_test)                \n",
    "\n",
    "            self.model.fit(train_pool)\n",
    "\n",
    "            predictions = self.model.predict(test_pool)\n",
    "            \n",
    "            # Запишем предсказания в датафрейм ratings. Чем выше ratings[user_id][movie], \n",
    "            # тем вероятнее пользователь user_id посмотрит фильм movie\n",
    "            self.ratings.loc[ind:min(ind + self.test_rows, len(self.test_df)) - 1, y_train.columns] = predictions.reshape(-1, 1)\n",
    "\n",
    "    def predict(self, df: pd.DataFrame, test_parts: int = 1) -> list:\n",
    "        '''\n",
    "        test_parts в данном методе - это количество частей, на которые мы будем делить тестовую часть. \n",
    "        Т.е. если test_parts == len(test_part), то для каждого пользователя из тестовой части будем отдельно обучать \n",
    "        модель, используя для обучения всех пользователей кроме текущего. Если test_parts == 1, \n",
    "        то модель будем обучать только один раз - на всех пользователях кроме тестовых.\n",
    "        '''\n",
    "\n",
    "        assert self.trained\n",
    "\n",
    "        self.test_df = df\n",
    "        self.test_rows = len(df) // test_parts\n",
    "        \n",
    "        # Создаем матрицу, в которой для каждой пары (user_id, movie) будет записано предсказание.\n",
    "        self.ratings = pd.DataFrame(columns=self.matrix.columns, index=range(len(self.test_df)))\n",
    "        \n",
    "        # Делаем предсказания по частям\n",
    "        for ind in range (0, len(self.test_df), self.test_rows):\n",
    "            self._predict(ind)\n",
    "        \n",
    "        # Если пользователь user_id посмотрел фильм movie, то считаем, что ratings[user_id][movie] = 0,\n",
    "        # чтобы не рекомендовать фильм, который пользователь уже посмотрел.\n",
    "        for user in range(len(self.test_df)):\n",
    "            user_id = self.test_df.iloc[user]['user_id']\n",
    "            self.ratings.iloc[user][list(self.grouped_df[self.grouped_df['user_id'] == user_id]['movie_id'])[0]] = 0\n",
    "            \n",
    "        # Выбираем для каждого пользователя 10 фильмов с наивысшей оценкой.\n",
    "        self.ratings = self.ratings.astype(float)\n",
    "        top_10_movies = self.ratings.apply(lambda row: row.nlargest(10).index.tolist(), axis=1)\n",
    "\n",
    "        return top_10_movies.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0eed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoosting(iterations = 5, learning_rate = 0.25, depth = 6)\n",
    "model.fit(train_part, items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d88d8a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mGradientBoosting.predict\u001b[1;34m(self, df, test_parts)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Делаем предсказания по частям\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_df), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_rows):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Если пользователь user_id посмотрел фильм movie, то считаем, что ratings[user_id][movie] = 0,\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# чтобы не рекомендовать фильм, который пользователь уже посмотрел.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_df)):\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mGradientBoosting._predict\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m    115\u001b[0m     train_pool \u001b[38;5;241m=\u001b[39m Pool(data\u001b[38;5;241m=\u001b[39mX_train, label\u001b[38;5;241m=\u001b[39my_train)\n\u001b[0;32m    116\u001b[0m     test_pool \u001b[38;5;241m=\u001b[39m Pool(data\u001b[38;5;241m=\u001b[39mX_test)                \n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(test_pool)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Запишем предсказания в датафрейм ratings. Чем выше ratings[user_id][movie], \u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# тем вероятнее пользователь user_id посмотрит фильм movie\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5874\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5875\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5876\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5017\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5066\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_part['gb_recs'] = model.predict(test_part, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9841f42b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gb_recs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gb_recs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_recommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_preds_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgb_recs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mevaluate_recommender\u001b[1;34m(df, model_preds_col, gt_col)\u001b[0m\n\u001b[0;32m     35\u001b[0m metric_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     38\u001b[0m     metric_values\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m---> 39\u001b[0m         (ndcg_metric(row[gt_col], \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_preds_col\u001b[49m\u001b[43m]\u001b[49m), recall_metric(row[gt_col], row[model_preds_col]))\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m metric_values]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m metric_values])}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gb_recs'"
     ]
    }
   ],
   "source": [
    "evaluate_recommender(df=test_part, model_preds_col=\"gb_recs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a599bcf",
   "metadata": {},
   "source": [
    "Модель показывает результат чуть лучше, чем если бы мы рекомендовали всем пользователям самые популярные фильмы. В отличие от предыдущей модели градиентный бустинг неявно учитывает популярность фильмов, ведь чем популярнее фильм, тем больше различных пользователей его посмотрело, а значит, в решающем дереве на него наложено меньше \"ограничений\". Также эта модель использует информацию о пользователях и обучается для каждого фильма отдельно. Кроме того, метрики вероятно можно улучшить, если оптимально подобрать гиперпараметры."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc684189",
   "metadata": {},
   "source": [
    "Теперь проанализируем предсказания моделей. Оценим, насколько точно модели повторяют распределение для 10 случайных фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b593aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = np.random.choice(train_part['movie_id'].value_counts().index.values[:500], size=10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2194fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train_part['movie_id'].value_counts()\n",
    "counts = value_counts[movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_gb = pd.DataFrame(np.concatenate(test_part['gb_recs'].tolist()))\n",
    "counts_gb = movies_gb[0].value_counts()\n",
    "counts_gb = counts_gb.reindex(movies, fill_value=0)\n",
    "counts_gb = counts_gb[movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76927bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_knn = pd.DataFrame(np.concatenate(test_part['knn_recs'].tolist()))\n",
    "counts_knn = movies_knn[0].value_counts()\n",
    "counts_knn = counts_knn.reindex(movies, fill_value=0)\n",
    "counts_knn = counts_knn[movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "sns.barplot(x=counts.index, y=counts.values, ax=axes[0])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].set_title('train_part')\n",
    "\n",
    "sns.barplot(x=counts_gb.index, y=counts_gb.values, ax=axes[1])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].set_title('Градиентный бустинг')\n",
    "\n",
    "sns.barplot(x=counts_knn.index, y=counts_knn.values, ax=axes[2])\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].set_title('KNN')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e7a84",
   "metadata": {},
   "source": [
    "По графикам видно, что градиентный бустинг более точно повторяет распределение фильмов, чем KNN. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
